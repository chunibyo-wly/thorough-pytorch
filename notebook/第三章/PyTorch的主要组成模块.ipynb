{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6ea800",
   "metadata": {},
   "source": [
    "# 3.2 基本配置\n",
    "对于一个PyTorch项目，我们需要导入一些Python常用的包来帮助我们快速实现功能。常见的包有os、numpy等，此外还需要调用PyTorch自身一些模块便于灵活使用，比如torch、torch.nn、torch.utils.data.Dataset、torch.utils.data.DataLoader、torch.optimizer等等。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- 在深度学习/机器学习中常用到的包\n",
    "- GPU的配置\n",
    "\n",
    "首先导入必须的包。注意这里**只是建议导入的包导入的方式**，可以采用不同的方案，比如涉及到表格信息的读入很可能用到pandas，对于不同的项目可能还需要导入一些更上层的包如cv2等。如果涉及可视化还会用到matplotlib、seaborn等。涉及到下游分析和指标计算也常用到sklearn。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5511fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2b04d",
   "metadata": {},
   "source": [
    "\n",
    "根据前面我们对深度学习任务的梳理，有如下几个超参数可以统一设置，方便后续调试时修改：\n",
    "\n",
    "- batch size\n",
    "- 初始学习率（初始）\n",
    "- 训练次数（max_epochs）\n",
    "- GPU配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd158eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# 批次的大小\n",
    "lr = 1e-4\n",
    "# 优化器的学习率\n",
    "max_epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419977de",
   "metadata": {},
   "source": [
    "除了直接将超参数设置在训练的代码里，我们也可以使用yaml、json，dict等文件来存储超参数，这样可以方便后续的调试和修改，这种方式也是常见的深度学习库（mmdetection，Paddledetection，detectron2）和一些AI Lab里面比较常见的一种参数设置方式。\n",
    "\n",
    "我们的数据和模型如果没有经过显式指明设备，默认会存储在CPU上，为了加速模型的训练，我们需要显式调用GPU，一般情况下GPU的设置有两种常见的方式：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17300c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案一：使用os.environ，这种情况如果使用GPU不需要设置\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 指明调用的GPU为0,1号\n",
    "\n",
    "# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") # 指明调用的GPU为1号\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad01f8",
   "metadata": {},
   "source": [
    "# 3.3 数据读入\n",
    "\n",
    "PyTorch数据读入是通过Dataset+DataLoader的方式完成的，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- PyTorch常见的数据读取方式\n",
    "- 构建自己的数据读取流程\n",
    "\n",
    "我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：\n",
    "\n",
    "- `__init__`: 用于向类中传入外部参数，同时定义样本集\n",
    "- `__getitem__`: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据\n",
    "- `__len__`: 用于返回数据集的样本数\n",
    "\n",
    "下面以cifar10数据集为例给出构建Dataset类的方式：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea55365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(train_path, transform=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path, transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375ce5b",
   "metadata": {},
   "source": [
    "\n",
    "这里使用了PyTorch自带的ImageFolder类的用于读取按一定结构存储的图片数据（path对应图片存放的目录，目录下包含若干子目录，每个子目录对应属于同一个类的图片）。\n",
    "\n",
    "其中`data_transform`可以对图像进行一定的变换，如翻转、裁剪等操作，可自己定义。这里我们会在下一章通过实战加以介绍并在[notebook](https://github.com/datawhalechina/thorough-pytorch/tree/main/notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B)中做了示例代码。\n",
    "\n",
    "这里我们给出一个自己定制Dataset的例子\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on the target.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca96297",
   "metadata": {},
   "source": [
    "其中，我们的标签类似于以下的形式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1.jpg, 0\n",
    "image2.jpg, 1\n",
    "......\n",
    "image9.jpg, 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babeba34",
   "metadata": {},
   "source": [
    "构建好Dataset后，就可以使用DataLoader来按批次读入数据了，实现代码如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ce8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf9692",
   "metadata": {},
   "source": [
    "\n",
    "其中:\n",
    "\n",
    "- batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数\n",
    "- num_workers：有多少个进程用于读取数据，Windows下该参数设置为0，Linux下常见的为4或者8，根据自己的电脑配置来设置\n",
    "- shuffle：是否将读入的数据打乱，一般在训练集中设置为True，验证集中设置为False\n",
    "- drop_last：对于样本最后一部分没有达到批次数的样本，使其不再参与训练\n",
    "\n",
    "这里可以看一下我们的加载的数据。PyTorch中的DataLoader的读取可以使用next和iter来完成\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6484228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(val_loader))\n",
    "print(images.shape)\n",
    "plt.imshow(images[0].transpose(1,2,0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffc3e2",
   "metadata": {},
   "source": [
    "\n",
    "# 3.4 模型构建\n",
    "人工智能的第三次浪潮受益于卷积神经网络的出现和BP反向传播算法的实现，随着深度学习的发展，研究人员研究出了许许多多的模型，PyTorch中神经网络构造一般是基于`nn.Module`类的模型来完成的，它让模型构造更加灵活。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- PyTorch中神经网络的构造方法\n",
    "- PyTorch中特殊层的构建\n",
    "- LeNet的PyTorch实现\n",
    "\n",
    "## 3.4.1 神经网络的构造\n",
    "\n",
    "`Module` 类是 `torch.nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算（正向传播）。下面的 MLP 类定义了一个具有两个隐藏层的多层感知机。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05765ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Linear(784, 256)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(256,10)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5249c0",
   "metadata": {},
   "source": [
    "\n",
    "以上的 MLP 类中⽆须定义反向传播函数。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 `backward` 函数。\n",
    "\n",
    "我们可以实例化 `MLP` 类得到模型变量 `net` 。下⾯的代码初始化 `net` 并传入输⼊数据 `X` 做一次前向计算。其中， `net(X)` 会调用 `MLP` 继承⾃自 `Module` 类的 `__call__` 函数，这个函数将调⽤用 `MLP` 类定义的`forward` 函数来完成前向计算。因此我们自己构造模型时需要明确定义模型的`forward`过程\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21532940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2,784) # 设置一个随机的输入张量\n",
    "net = MLP() # 实例化模型\n",
    "print(net) # 打印模型\n",
    "net(X) # 前向计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d057a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP(\n",
    "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
    "  (act): ReLU()\n",
    "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
    ")\n",
    "tensor([[ 0.0149, -0.2641, -0.0040,  0.0945, -0.1277, -0.0092,  0.0343,  0.0627,\n",
    "         -0.1742,  0.1866],\n",
    "        [ 0.0738, -0.1409,  0.0790,  0.0597, -0.1572,  0.0479, -0.0519,  0.0211,\n",
    "         -0.1435,  0.1958]], grad_fn=<AddmmBackward>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263d0ed",
   "metadata": {},
   "source": [
    "\n",
    "注意，这里并没有将 `Module` 类命名为 `Layer` (层)或者 `Model` (模型)之类的名字，这是因为该类是一个可供⾃由组建的部件。它的子类既可以是⼀个层(如PyTorch提供的 Linear 类)，⼜可以是一个模型(如这里定义的 MLP 类)，或者是模型的⼀个部分。\n",
    "\n",
    "## 3.4.2 神经网络中常见的层\n",
    "\n",
    "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。这里我们会介绍如何使用 `Module` 来自定义层，从而可以被反复调用。\n",
    "\n",
    "- **不含模型参数的层**\n",
    "\n",
    "我们先介绍如何定义一个不含模型参数的自定义层。下⾯构造的 `MyLayer` 类通过继承 `Module` 类自定义了一个**将输入减掉均值后输出**的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c89c7",
   "metadata": {},
   "source": [
    "\n",
    "测试，实例化该层，然后做前向计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MyLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9d1ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([-2., -1.,  0.,  1.,  2.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f260810",
   "metadata": {},
   "source": [
    "\n",
    "- **含模型参数的层**\n",
    "\n",
    "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
    "\n",
    "`Parameter` 类其实是 `Tensor` 的子类，如果一个 `Tensor` 是 `Parameter` ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 `Parameter` ，除了直接定义成 `Parameter` 类外，还可以使⽤ `ParameterList` 和 `ParameterDict` 分别定义参数的列表和字典。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1228a3c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a74934",
   "metadata": {},
   "source": [
    "\n",
    "下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。\n",
    "\n",
    "- **二维卷积层**\n",
    "\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a70d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 卷积运算（二维互相关）\n",
    "def corr2d(X, K): \n",
    "    h, w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab010f",
   "metadata": {},
   "source": [
    "\n",
    "卷积窗口形状为 $p \\times q$ 的卷积层称为 $p \\times q$  卷积层。同样，  $p \\times q$ 卷积或 $p \\times q$ 卷积核说明卷积核的高和宽分别为 $p$ 和 $q$，一般情况下，$p=q$。\n",
    "\n",
    "填充(padding)是指在输⼊高和宽的两侧填充元素(通常是0元素)。\n",
    "\n",
    "下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一个高和宽为8的输入，我们发现输出的高和宽也是8。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3218c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1)代表批量大小和通道数\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:]) # 排除不关心的前两维:批量和通道\n",
    "\n",
    "\n",
    "# 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)\n",
    "\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358cc6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a525b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([8, 8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606daaa5",
   "metadata": {},
   "source": [
    "\n",
    "当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c4087",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([8, 8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60483d85",
   "metadata": {},
   "source": [
    "在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23e016",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ece94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([2, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6231c",
   "metadata": {},
   "source": [
    "\n",
    "填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。\n",
    "\n",
    "步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。\n",
    "\n",
    "- **池化层**\n",
    "\n",
    "池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的属性（均值、最大值等）。常见的池化包括最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。\n",
    "\n",
    "下面把池化层的前向计算实现在`pool2d`函数里。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589e9c1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e650f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034da2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[4., 5.],\n",
    "\t[7., 8.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba2b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10979e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d(X, (2, 2), 'avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd13f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd404824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[2., 3.],\n",
    "\t[5., 6.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a787568",
   "metadata": {},
   "source": [
    "\n",
    "我们可以使用`torch.nn`包来构建神经网络。我们已经介绍了`autograd`包，`nn`包则依赖于`autograd`包来定义模型并对它们求导。一个`nn.Module`包含各个层和一个`forward(input)`方法，该方法返回`output`。\n",
    "\n",
    "## 3.4.3 模型示例\n",
    "\n",
    "- **LeNet**\n",
    "\n",
    "![3.4.1](./figures/3.4.1.png)\n",
    "\n",
    "这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。\n",
    "\n",
    "一个神经网络的典型训练过程如下：\n",
    "\n",
    "1. 定义包含一些可学习参数(或者叫权重）的神经网络\n",
    "2. 在输入数据集上迭代\n",
    "3. 通过网络处理输入\n",
    "4. 计算 loss (输出和正确答案的距离）\n",
    "5. 将梯度反向传播给网络的参数\n",
    "6. 更新网络的权重，一般使用一个简单的规则：`weight = weight - learning_rate * gradient`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像channel：1；输出channel：6；5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2x2 Max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果是方阵,则可以只使用一个数字进行定义\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除去批处理维度的其他所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca3032",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80368cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net(\n",
    "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
    "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
    "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0c2cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "我们只需要定义 `forward` 函数，`backward`函数会在使用`autograd`时自动定义，`backward`函数用来计算导数。我们可以在 `forward` 函数中使用任何针对张量的操作和计算。\n",
    "\n",
    "一个模型的可学习参数可以通过`net.parameters()`返回\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f31155",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1的权重\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966462b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "10\n",
    "torch.Size([6, 1, 5, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab504537",
   "metadata": {},
   "source": [
    "让我们尝试一个随机的 32x32 的输入。注意:这个网络 (LeNet）的期待输入是 32x32 的张量。如果使用 MNIST 数据集来训练这个网络，要把图片大小重新调整到 32x32。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83245562",
   "metadata": {},
   "source": [
    "\n",
    "清零所有参数的梯度缓存，然后进行随机梯度的反向传播：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524361bc",
   "metadata": {},
   "source": [
    "\n",
    "注意：`torch.nn`只支持小批量处理 (mini-batches）。整个 `torch.nn` 包只支持小批量样本的输入，不支持单个样本的输入。比如，`nn.Conv2d` 接受一个4维的张量，即`nSamples x nChannels x Height x Width `如果是一个单独的样本，只需要使用`input.unsqueeze(0)` 来添加一个“假的”批大小维度。\n",
    "\n",
    "- `torch.Tensor` - 一个多维数组，支持诸如`backward()`等的自动求导操作，同时也保存了张量的梯度。\n",
    "\n",
    "- `nn.Module `- 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。\n",
    "\n",
    "- `nn.Parameter `- 张量的一种，当它作为一个属性分配给一个`Module`时，它会被自动注册为一个参数。\n",
    "\n",
    "- `autograd.Function` - 实现了自动求导前向和反向传播的定义，每个`Tensor`至少创建一个`Function`节点，该节点连接到创建`Tensor`的函数并对其历史进行编码。\n",
    "\n",
    "下面再介绍一个比较基础的案例AlexNet\n",
    "\n",
    "- **AlexNet**\n",
    "\n",
    "![3.4.2](./figures/3.4.2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a83e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2), # kernel_size, stride\n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "            # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2119781",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7815a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c42c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet(\n",
    "  (conv): Sequential(\n",
    "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (4): ReLU()\n",
    "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (7): ReLU()\n",
    "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (9): ReLU()\n",
    "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (11): ReLU()\n",
    "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (fc): Sequential(\n",
    "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Dropout(p=0.5)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU()\n",
    "    (5): Dropout(p=0.5)\n",
    "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8cfdf",
   "metadata": {},
   "source": [
    "# 3.5 模型初始化\n",
    "在深度学习模型的训练中，权重的初始值极为重要。一个好的初始值，会使模型收敛速度提高，使模型准确率更精确。一般情况下，我们不使用全0初始值训练网络。为了利于训练和减少收敛时间，我们需要对模型进行合理的初始化。PyTorch也在`torch.nn.init`中为我们提供了常用的初始化方法。\n",
    "通过本章学习，你将学习到以下内容：\n",
    "- 常见的初始化函数\n",
    "- 初始化函数的使用\n",
    "\n",
    "## torch.nn.init内容\n",
    "通过访问torch.nn.init的官方文档[链接](https://pytorch.org/docs/stable/nn.init.html) ，我们发现`torch.nn.init`提供了以下初始化方法：\n",
    "1 . `torch.nn.init.uniform_`(tensor, a=0.0, b=1.0)\n",
    "2 . `torch.nn.init.normal_`(tensor, mean=0.0, std=1.0)\n",
    "3 . `torch.nn.init.constant_`(tensor, val)\n",
    "4 . `torch.nn.init.ones_`(tensor)\n",
    "5 . `torch.nn.init.zeros_`(tensor)\n",
    "6 . `torch.nn.init.eye_`(tensor)\n",
    "7 . `torch.nn.init.dirac_`(tensor, groups=1)\n",
    "8 . `torch.nn.init.xavier_uniform_`(tensor, gain=1.0)\n",
    "9 . `torch.nn.init.xavier_normal_`(tensor, gain=1.0)\n",
    "10 . `torch.nn.init.kaiming_uniform_`(tensor, a=0, mode='fan__in', nonlinearity='leaky_relu')\n",
    "11 . `torch.nn.init.kaiming_normal_`(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "12 . `torch.nn.init.orthogonal_`(tensor, gain=1)\n",
    "13 . `torch.nn.init.sparse_`(tensor, sparsity, std=0.01)\n",
    "14 .  `torch.nn.init.calculate_gain`(nonlinearity, param=None)\n",
    "关于计算增益如下表：\n",
    "|nonlinearity|gain|\n",
    "| ---- | ---- |\n",
    "|Linear/Identity|1|\n",
    "|Conv{1,2,3}D|1|\n",
    "|Sigmod|1|\n",
    "|Tanh|5/3|\n",
    "|ReLU|sqrt(2)|\n",
    "|Leaky Relu|sqrt(2/1+neg_slop^2)|\n",
    "\n",
    "我们可以发现这些函数除了`calculate_gain`，所有函数的后缀都带有下划线，意味着这些函数将会直接原地更改输入张量的值。\n",
    "\n",
    "## torch.nn.init使用\n",
    "我们通常会根据实际模型来使用`torch.nn.init`进行初始化，通常使用`isinstance()`来进行判断模块（回顾3.4模型构建）属于什么类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0005e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(1,3,3)\n",
    "linear = nn.Linear(10,1)\n",
    "\n",
    "isinstance(conv,nn.Conv2d) # 判断conv是否是nn.Conv2d类型\n",
    "isinstance(linear,nn.Conv2d) # 判断linear是否是nn.Conv2d类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc3fa2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a83680",
   "metadata": {},
   "outputs": [],
   "source": [
    "True\n",
    "False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed045092",
   "metadata": {},
   "source": [
    "对于不同的类型层，我们就可以设置不同的权值初始化的方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看随机初始化的conv参数\n",
    "conv.weight.data\n",
    "# 查看linear的参数\n",
    "linear.weight.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e4ed1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[[ 0.1174,  0.1071,  0.2977],\n",
    "          [-0.2634, -0.0583, -0.2465],\n",
    "          [ 0.1726, -0.0452, -0.2354]]],\n",
    "        [[[ 0.1382,  0.1853, -0.1515],\n",
    "          [ 0.0561,  0.2798, -0.2488],\n",
    "          [-0.1288,  0.0031,  0.2826]]],\n",
    "        [[[ 0.2655,  0.2566, -0.1276],\n",
    "          [ 0.1905, -0.1308,  0.2933],\n",
    "          [ 0.0557, -0.1880,  0.0669]]]])\n",
    "\n",
    "tensor([[-0.0089,  0.1186,  0.1213, -0.2569,  0.1381,  0.3125,  0.1118, -0.0063, -0.2330,  0.1956]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74558abd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对conv进行kaiming初始化\n",
    "torch.nn.init.kaiming_normal_(conv.weight.data)\n",
    "conv.weight.data\n",
    "# 对linear进行常数初始化\n",
    "torch.nn.init.constant_(linear.weight.data,0.3)\n",
    "linear.weight.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b739b38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[[ 0.3249, -0.0500,  0.6703],\n",
    "          [-0.3561,  0.0946,  0.4380],\n",
    "          [-0.9426,  0.9116,  0.4374]]],\n",
    "        [[[ 0.6727,  0.9885,  0.1635],\n",
    "          [ 0.7218, -1.2841, -0.2970],\n",
    "          [-0.9128, -0.1134, -0.3846]]],\n",
    "        [[[ 0.2018,  0.4668, -0.0937],\n",
    "          [-0.2701, -0.3073,  0.6686],\n",
    "          [-0.3269, -0.0094,  0.3246]]]])\n",
    "tensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,0.3000]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099cbbe",
   "metadata": {},
   "source": [
    "##  初始化函数的封装\n",
    "人们常常将各种初始化方法定义为一个`initialize_weights()`的函数并在模型初始后进行使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "\tfor m in model.modules():\n",
    "\t\t# 判断是否属于Conv2d\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\ttorch.nn.init.zeros_(m.weight.data)\n",
    "\t\t\t# 判断是否有偏置\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.constant_(m.bias.data,0.3)\n",
    "\t\telif isinstance(m, nn.Linear):\n",
    "\t\t\ttorch.nn.init.normal_(m.weight.data, 0.1)\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.zeros_(m.bias.data)\n",
    "\t\telif isinstance(m, nn.BatchNorm2d):\n",
    "\t\t\tm.weight.data.fill_(1) \t\t \n",
    "\t\t\tm.bias.data.zeros_()\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98690f11",
   "metadata": {},
   "source": [
    "这段代码流程是遍历当前模型的每一层，然后判断各层属于什么类型，然后根据不同类型层，设定不同的权值初始化方法。我们可以通过下面的例程进行一个简短的演示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6126cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的定义\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Conv2d(1,1,3)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(10,1)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp.hidden.weight.data)\n",
    "print(\"-------初始化-------\")\n",
    "\n",
    "mlp.apply(initialize_weights)\n",
    "# 或者initialize_weights(mlp)\n",
    "print(mlp.hidden.weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfec5f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc77868",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[[ 0.3069, -0.1865,  0.0182],\n",
    "          [ 0.2475,  0.3330,  0.1352],\n",
    "          [-0.0247, -0.0786,  0.1278]]]])\n",
    "\"-------初始化-------\"\n",
    "tensor([[[[0., 0., 0.],\n",
    "          [0., 0., 0.],\n",
    "          [0., 0., 0.]]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fbe63",
   "metadata": {},
   "source": [
    "# 3.6 损失函数\n",
    "\n",
    "在深度学习广为使用的今天，我们可以在脑海里清晰的知道，一个模型想要达到很好的效果需要**学习**，也就是我们常说的训练。一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。\n",
    "\n",
    "![](./figures/3.5.1lossfunciton.png)\n",
    "\n",
    "所以在PyTorch中，损失函数是必不可少的。它是数据输入到模型当中，产生的结果与真实标签的评价指标，我们的模型可以按照损失函数的目标来做出改进。\n",
    "\n",
    "下面我们将开始探索PyTorch的所拥有的损失函数。这里将列出PyTorch中常用的损失函数（一般通过torch.nn调用），并详细介绍每个损失函数的功能介绍、数学公式和调用代码。当然，PyTorch的损失函数还远不止这些，在解决实际问题的过程中需要进一步探索、借鉴现有工作，或者设计自己的损失函数。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- 在深度学习中常见的损失函数及其定义方式\n",
    "- PyTorch中损失函数的调用\n",
    "\n",
    "\n",
    "## 3.5.1 二分类交叉熵损失函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab20fc",
   "metadata": {},
   "source": [
    "\n",
    "**功能**：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。\n",
    "\n",
    "**主要参数**：\n",
    "\n",
    "`weight`:每个类别的loss设置权值\n",
    "\n",
    "`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "\n",
    "`reduce`:数据类型为bool，为True时，loss的返回是标量。\n",
    "\n",
    "计算公式如下：\n",
    "$\n",
    "\\ell(x, y)=\\left\\{\\begin{array}{ll}\n",
    "\\operatorname{mean}(L), & \\text { if reduction }=\\text { 'mean' } \\\\\n",
    "\\operatorname{sum}(L), & \\text { if reduction }=\\text { 'sum' }\n",
    "\\end{array}\\right.\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb1341",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28895b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BCELoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4052e",
   "metadata": {},
   "source": [
    "\n",
    "    BCELoss损失函数的计算结果为 tensor(0.5732, grad_fn=<BinaryCrossEntropyBackward>)\n",
    "\n",
    "\n",
    "## 3.5.2 交叉熵损失函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3907642",
   "metadata": {},
   "source": [
    "\n",
    "**功能**：计算交叉熵函数\n",
    "\n",
    "**主要参数**：  \n",
    "\n",
    "`weight`:每个类别的loss设置权值。\n",
    "\n",
    "`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "\n",
    "`ignore_index`:忽略某个类的损失函数。\n",
    "\n",
    "`reduce`:数据类型为bool，为True时，loss的返回是标量。\n",
    "\n",
    "计算公式如下：\n",
    "$\n",
    "\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce15c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd808a",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185a206",
   "metadata": {},
   "source": [
    "\n",
    "    tensor(2.0115, grad_fn=<NllLossBackward>)\n",
    "\n",
    "## 3.5.3 L1损失函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5e421",
   "metadata": {},
   "source": [
    "\n",
    "**功能：** 计算输出`y`和真实标签`target`之间的差值的绝对值。\n",
    "\n",
    "我们需要知道的是，`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。\n",
    "mean：加权平均，返回标量。 \n",
    "如果选择`none`，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。\n",
    "\n",
    "**计算公式如下：**\n",
    "$\n",
    "L_{n} = |x_{n}-y_{n}|\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb8643",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('L1损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fec46",
   "metadata": {},
   "source": [
    "\n",
    "    L1损失函数的计算结果为 tensor(1.5729, grad_fn=<L1LossBackward>)\n",
    "\n",
    "\n",
    "## 3.5.4 MSE损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8f891",
   "metadata": {},
   "source": [
    "**功能：** 计算输出`y`和真实标签`target`之差的平方。\n",
    "\n",
    "和`L1Loss`一样，`MSELoss`损失函数中，`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。默认计算方式是求平均。\n",
    "\n",
    "**计算公式如下：**\n",
    "\n",
    "$\n",
    "l_{n}=\\left(x_{n}-y_{n}\\right)^{2}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42b90b",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40adbfb",
   "metadata": {},
   "source": [
    "\n",
    "    MSE损失函数的计算结果为 tensor(1.6968, grad_fn=<MseLossBackward>)\n",
    "\n",
    "\n",
    "## 3.5.5 平滑L1 (Smooth L1)损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7956d",
   "metadata": {},
   "source": [
    "**功能：** L1的平滑输出，其功能是减轻离群点带来的影响\n",
    "\n",
    "`reduction`参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。\n",
    "sum：所有元素求和，返回标量。默认计算方式是求平均。\n",
    "\n",
    "**提醒：** 之后的损失函数中，关于`reduction` 这个参数依旧会存在。所以，之后就不再单独说明。\n",
    "\n",
    "**计算公式如下：**\n",
    "$\n",
    "\\operatorname{loss}(x, y)=\\frac{1}{n} \\sum_{i=1}^{n} z_{i}\n",
    "$\n",
    "其中，\n",
    "$\n",
    "z_{i}=\\left\\{\\begin{array}{ll}\n",
    "0.5\\left(x_{i}-y_{i}\\right)^{2}, & \\text { if }\\left|x_{i}-y_{i}\\right|<1 \\\\\n",
    "\\left|x_{i}-y_{i}\\right|-0.5, & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f919026",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SmoothL1Loss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff52d0",
   "metadata": {},
   "source": [
    "\n",
    "    SmoothL1Loss损失函数的计算结果为 tensor(0.7808, grad_fn=<SmoothL1LossBackward>)\n",
    "\n",
    "**平滑L1与L1的对比**\n",
    "\n",
    "这里我们通过可视化两种损失函数曲线来对比平滑L1和L1两种损失函数的区别。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(-10, 10, steps=5000)\n",
    "target = torch.zeros_like(inputs)\n",
    "\n",
    "loss_f_smooth = nn.SmoothL1Loss(reduction='none')\n",
    "loss_smooth = loss_f_smooth(inputs, target)\n",
    "loss_f_l1 = nn.L1Loss(reduction='none')\n",
    "loss_l1 = loss_f_l1(inputs,target)\n",
    "\n",
    "plt.plot(inputs.numpy(), loss_smooth.numpy(), label='Smooth L1 Loss')\n",
    "plt.plot(inputs.numpy(), loss_l1, label='L1 loss')\n",
    "plt.xlabel('x_i - y_i')\n",
    "plt.ylabel('loss value')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0fb06",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![png](./figures/3.5.2.png)\n",
    "\n",
    "\n",
    "可以看出，对于`smoothL1`来说，在 0 这个尖端处，过渡更为平滑。\n",
    "\n",
    "## 3.5.6 目标泊松分布的负对数似然损失\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52600b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f236d",
   "metadata": {},
   "source": [
    "**功能：** 泊松分布的负对数似然损失函数\n",
    "\n",
    "**主要参数：**\n",
    "\n",
    "`log_input`：输入是否为对数形式，决定计算公式。\n",
    "\n",
    "`full`：计算所有 loss，默认为 False。\n",
    "\n",
    "`eps`：修正项，避免 input 为 0 时，log(input) 为 nan 的情况。\n",
    "\n",
    "**数学公式：**\n",
    "\n",
    "- 当参数`log_input=True`：\n",
    "$\n",
    "\\operatorname{loss}\\left(x_{n}, y_{n}\\right)=e^{x_{n}}-x_{n} \\cdot y_{n}\n",
    "$\n",
    "\n",
    "\n",
    "- 当参数`log_input=False`：\n",
    "\n",
    "    $\n",
    "    \\operatorname{loss}\\left(x_{n}, y_{n}\\right)=x_{n}-y_{n} \\cdot \\log \\left(x_{n}+\\text { eps }\\right)\n",
    "    $\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = torch.randn(5, 2, requires_grad=True)\n",
    "target = torch.randn(5, 2)\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40197833",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PoissonNLLLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49e47a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PoissonNLLLoss损失函数的计算结果为 tensor(0.7358, grad_fn=<MeanBackward0>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bcac3",
   "metadata": {},
   "source": [
    "\n",
    "## 3.5.7 KL散度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ef394",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f76549",
   "metadata": {},
   "source": [
    "**功能：** 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "`reduction`：计算模式，可为 `none`/`sum`/`mean`/`batchmean`。\n",
    "\n",
    "    none：逐个元素计算。\n",
    "    \n",
    "    sum：所有元素求和，返回标量。\n",
    "    \n",
    "    mean：加权平均，返回标量。\n",
    "    \n",
    "    batchmean：batchsize 维度求平均值。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "D_{\\mathrm{KL}}(P, Q)=\\mathrm{E}_{X \\sim P}\\left[\\log \\frac{P(X)}{Q(X)}\\right] &=\\mathrm{E}_{X \\sim P}[\\log P(X)-\\log Q(X)] \\\\\n",
    "&=\\sum_{i=1}^{n} P\\left(x_{i}\\right)\\left(\\log P\\left(x_{i}\\right)-\\log Q\\left(x_{i}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "loss = nn.KLDivLoss()\n",
    "output = loss(inputs,target)\n",
    "\n",
    "print('KLDivLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaf29b",
   "metadata": {},
   "source": [
    "\n",
    "    KLDivLoss损失函数的计算结果为 tensor(-0.3335)\n",
    "\n",
    "\n",
    "## 3.5.8 MarginRankingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75076451",
   "metadata": {},
   "source": [
    "**功能：** 计算两个向量之间的相似度，用于排序任务。该方法用于计算两组数据之间的差异。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "`margin`：边界值，$x_{1}$ 与$x_{2}$ 之间的差异值。\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "\\operatorname{loss}(x 1, x 2, y)=\\max (0,-y *(x 1-x 2)+\\operatorname{margin})\n",
    "$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e433ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "\n",
    "print('MarginRankingLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c3bcd",
   "metadata": {},
   "source": [
    "\n",
    "    MarginRankingLoss损失函数的计算结果为 tensor(0.7740, grad_fn=<MeanBackward0>)\n",
    "\n",
    "\n",
    "## 3.5.9 多标签边界损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81714a5",
   "metadata": {},
   "source": [
    "\n",
    "**功能：** 对于多标签分类问题计算损失函数。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "$\n",
    "\\operatorname{loss}(x, y)=\\sum_{i j} \\frac{\\max (0,1-x[y[j]]-x[i])}{x \\cdot \\operatorname{size}(0)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}\n",
    "\\text { 其中, } i=0, \\ldots, x \\cdot \\operatorname{size}(0), j=0, \\ldots, y \\cdot \\operatorname{size}(0), \\text { 对于所有的 } i \\text { 和 } j \\text {, 都有 } y[j] \\geq 0 \\text { 并且 }\\\\\n",
    "i \\neq y[j]\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n",
    "# for target y, only consider labels 3 and 0, not after label -1\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])# 真实的分类是，第3类和第0类\n",
    "output = loss(x, y)\n",
    "\n",
    "print('MultiLabelMarginLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b7cad",
   "metadata": {},
   "source": [
    "\n",
    "    MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)\n",
    "\n",
    "\n",
    "## 3.5.10 二分类损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887973d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')torch.nn.(size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456cb97",
   "metadata": {},
   "source": [
    "**功能：** 计算二分类的 logistic 损失。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "\\operatorname{loss}(x, y)=\\sum_{i} \\frac{\\log (1+\\exp (-y[i] \\cdot x[i]))}{x \\cdot \\operatorname{nelement}()}\n",
    "$\n",
    "\n",
    "$\n",
    "\\\n",
    "\\text { 其中, } x . \\text { nelement() 为输入 } x \\text { 中的样本个数。注意这里 } y \\text { 也有 } 1 \\text { 和 }-1 \\text { 两种模式。 }\n",
    "\\\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])  # 两个样本，两个神经元\n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)  # 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签\n",
    "\n",
    "loss_f = nn.SoftMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "\n",
    "print('SoftMarginLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784011cf",
   "metadata": {},
   "source": [
    "\n",
    "    SoftMarginLoss损失函数的计算结果为 tensor(0.6764)\n",
    "\n",
    "\n",
    "## 3.5.11 多分类的折页损失\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f79df",
   "metadata": {},
   "source": [
    "**功能：** 计算多分类的折页损失\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "`p：`可选 1 或 2。\n",
    "\n",
    "`weight`：各类别的 loss 设置权值。\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "\\operatorname{loss}(x, y)=\\frac{\\sum_{i} \\max (0, \\operatorname{margin}-x[y]+x[i])^{p}}{x \\cdot \\operatorname{size}(0)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}\n",
    "\\text { 其中, } x \\in\\{0, \\ldots, x \\cdot \\operatorname{size}(0)-1\\}, y \\in\\{0, \\ldots, y \\cdot \\operatorname{size}(0)-1\\} \\text {, 并且对于所有的 } i \\text { 和 } j \\text {, }\\\\\n",
    "\\text { 都有 } 0 \\leq y[j] \\leq x \\cdot \\operatorname{size}(0)-1, \\text { 以及 } i \\neq y[j] \\text { 。 }\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954be352",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]]) \n",
    "target = torch.tensor([0, 1], dtype=torch.long) \n",
    "\n",
    "loss_f = nn.MultiMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "\n",
    "print('MultiMarginLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57ab53",
   "metadata": {},
   "source": [
    "\n",
    "    MultiMarginLoss损失函数的计算结果为 tensor(0.6000)\n",
    "\n",
    "\n",
    "## 3.5.12 三元组损失\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b24ed5",
   "metadata": {},
   "source": [
    "**功能：** 计算三元组损失。\n",
    "\n",
    "**三元组:** 这是一种数据的存储或者使用格式。<实体1，关系，实体2>。在项目中，也可以表示为< `anchor`, `positive examples` , `negative examples`>\n",
    "\n",
    "在这个损失函数中，我们希望去`anchor`的距离更接近`positive examples`，而远离`negative examples `\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "`p：`可选 1 或 2。\n",
    "\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "L(a, p, n)=\\max \\left\\{d\\left(a_{i}, p_{i}\\right)-d\\left(a_{i}, n_{i}\\right)+\\operatorname{margin}, 0\\right\\}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text { 其中, } d\\left(x_{i}, y_{i}\\right)=\\left\\|\\mathbf{x}_{i}-\\mathbf{y}_{i}\\right\\|_{\\text {・ }}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3accb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "output = triplet_loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "print('TripletMarginLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6cc52",
   "metadata": {},
   "source": [
    "\n",
    "    TripletMarginLoss损失函数的计算结果为 tensor(1.1667, grad_fn=<MeanBackward0>)\n",
    "\n",
    "\n",
    "## 3.5.13 HingEmbeddingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8790f0",
   "metadata": {},
   "source": [
    "**功能：** 对输出的embedding结果做Hing损失计算\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "\n",
    "`margin`：边界值\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "l_{n}=\\left\\{\\begin{array}{ll}\n",
    "x_{n}, & \\text { if } y_{n}=1 \\\\\n",
    "\\max \\left\\{0, \\Delta-x_{n}\\right\\}, & \\text { if } y_{n}=-1\n",
    "\\end{array}\\right.\n",
    "$\n",
    "**注意事项：** 输入x应为两个输入之差的绝对值。\n",
    "\n",
    "可以这样理解，让个输出的是正例yn=1,那么loss就是x，如果输出的是负例y=-1，那么输出的loss就是要做一个比较。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.HingeEmbeddingLoss()\n",
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "output = loss_f(inputs,target)\n",
    "\n",
    "print('HingEmbeddingLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915e0f3",
   "metadata": {},
   "source": [
    "\n",
    "    HingEmbeddingLoss损失函数的计算结果为 tensor(0.7667)\n",
    "\n",
    "\n",
    "## 3.5.14 余弦相似度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a678a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcde513",
   "metadata": {},
   "source": [
    "**功能：** 对两个向量做余弦相似度\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "\n",
    "`margin`：可取值[-1,1] ，推荐为[0,0.5] 。\n",
    "\n",
    "\n",
    "**计算公式：**\n",
    "\n",
    "$\n",
    "\\operatorname{loss}(x, y)=\\left\\{\\begin{array}{ll}\n",
    "1-\\cos \\left(x_{1}, x_{2}\\right), & \\text { if } y=1 \\\\\n",
    "\\max \\left\\{0, \\cos \\left(x_{1}, x_{2}\\right)-\\text { margin }\\right\\}, & \\text { if } y=-1\n",
    "\\end{array}\\right.\n",
    "$\n",
    "其中,\n",
    "$\n",
    "\\cos (\\theta)=\\frac{A \\cdot B}{\\|A\\|\\|B\\|}=\\frac{\\sum_{i=1}^{n} A_{i} \\times B_{i}}{\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}\\right)^{2}} \\times \\sqrt{\\sum_{i=1}^{n}\\left(B_{i}\\right)^{2}}}\n",
    "$\n",
    "\n",
    "\n",
    "这个损失函数应该是最广为人知的。对于两个向量，做余弦相似度。将余弦相似度作为一个距离的计算方式，如果两个向量的距离近，则损失函数值小，反之亦然。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CosineEmbeddingLoss()\n",
    "inputs_1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "inputs_2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "target = torch.tensor([1, -1], dtype=torch.float)\n",
    "output = loss_f(inputs_1,inputs_2,target)\n",
    "\n",
    "print('CosineEmbeddingLoss损失函数的计算结果为',output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f50fb",
   "metadata": {},
   "source": [
    "\n",
    "    CosineEmbeddingLoss损失函数的计算结果为 tensor(0.5000)\n",
    "\n",
    "\n",
    "## 3.5.15 CTC损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422a7d2",
   "metadata": {},
   "source": [
    "**功能：** 用于解决时序类数据的分类\n",
    "\n",
    "计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 \"多对一\"，这就限制了目标序列的长度，使其必须是≤输入长度。\n",
    "\n",
    "**主要参数:** \n",
    "\n",
    "\n",
    "`reduction`：计算模式，可为 none/sum/mean。\n",
    "\n",
    "\n",
    "`blank`：blank label。\n",
    "\n",
    "\n",
    "`zero_infinity`：无穷大的值或梯度值为 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c105fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target are to be padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch (padding length)\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "print('CTCLoss损失函数的计算结果为',loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a8018",
   "metadata": {},
   "source": [
    "# 3.7 训练和评估\n",
    "我们在完成了模型的训练后，需要在测试集/验证集上完成模型的验证，以确保我们的模型具有泛化能力、不会出现过拟合等问题。在PyTorch中，训练和评估的流程是一致的，只是在训练过程中需要将模型的参数进行更新，而在评估过程中则不需要更新参数。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- PyTorch的训练/评估模式的开启\n",
    "- 完整的训练/评估流程\n",
    "\n",
    "\n",
    "完成了上述设定后就可以加载数据开始训练模型了。首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7eb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()   # 训练状态\n",
    "model.eval()   # 验证/测试状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd4d8c",
   "metadata": {},
   "source": [
    "\n",
    "我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in train_loader:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032845c5",
   "metadata": {},
   "source": [
    "\n",
    "之后将数据放到GPU上用于后续计算，此处以.cuda()为例\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = data.cuda(), label.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de3b30",
   "metadata": {},
   "source": [
    "\n",
    "开始用当前批次数据做训练时，应当先将优化器的梯度置零：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73244853",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffe27a",
   "metadata": {},
   "source": [
    "\n",
    "之后将data送入模型中训练：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1881c",
   "metadata": {},
   "source": [
    "\n",
    "根据预先定义的criterion计算损失函数：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91754c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617f6a3",
   "metadata": {},
   "source": [
    "\n",
    "将loss反向传播回网络：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6917ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6d885",
   "metadata": {},
   "source": [
    "\n",
    "使用优化器更新模型参数：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3e5da",
   "metadata": {},
   "source": [
    "\n",
    "这样一个训练过程就完成了，后续还可以计算模型准确率等指标，这部分会在下一节的图像分类实战中加以介绍。\n",
    "\n",
    "验证/测试的流程基本与训练过程一致，不同点在于：\n",
    "\n",
    "- 需要预先设置torch.no_grad，以及将model调至eval模式\n",
    "- 不需要将优化器的梯度置零\n",
    "- 不需要将loss反向回传到网络\n",
    "- 不需要更新optimizer\n",
    "\n",
    "一个完整的图像分类的训练过程如下所示：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\t\tprint('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a82ec6",
   "metadata": {},
   "source": [
    "\n",
    "对应的，一个完整图像分类的验证过程如下所示：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083538db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47617c48",
   "metadata": {},
   "source": [
    "对于图像分类任务，我们还可以使用sklearn.metrics中的classification_report函数来计算模型的准确率、召回率、F1值等指标，如下所示：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\"\"\"\n",
    "将下方代码的labels和preds替换为模型预测出来的所有label和preds，\n",
    "target_names替换为类别名称，\n",
    "既可得到模型的分类报告\n",
    "\"\"\"\n",
    "print(classification_report(labels.cpu(), preds.cpu(), target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4be124",
   "metadata": {},
   "source": [
    "# 3.9 PyTorch优化器\n",
    "\n",
    "深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，只不过这个最优解是一个矩阵，而如何快速求得这个最优解是深度学习研究的一个重点，以经典的resnet-50为例，它大约有2000万个系数需要进行计算，那么我们如何计算出这么多系数，有以下两种方法：\n",
    "\n",
    "1. 第一种是直接暴力穷举一遍参数，这种方法从理论上行得通，但是实施上可能性基本为0，因为参数量过于庞大。\n",
    "2. 为了使求解参数过程更快，人们提出了第二种办法，即BP+优化器逼近求解。\n",
    "\n",
    "因此，优化器是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- 了解PyTorch的优化器\n",
    "- 学会使用PyTorch提供的优化器进行优化\n",
    "- 优化器的属性和构造\n",
    "- 优化器的对比\n",
    "\n",
    "\n",
    "## 3.9.1 PyTorch提供的优化器\n",
    "\n",
    "PyTorch很人性化的给我们提供了一个优化器的库`torch.optim`，在这里面提供了多种优化器。\n",
    "\n",
    "+ torch.optim.SGD \n",
    "+ torch.optim.ASGD\n",
    "+ torch.optim.Adadelta\n",
    "+ torch.optim.Adagrad\n",
    "+ torch.optim.Adam\n",
    "+ torch.optim.AdamW\n",
    "+ torch.optim.Adamax\n",
    "+ torch.optim.RAdam\n",
    "+ torch.optim.NAdam\n",
    "+ torch.optim.SparseAdam\n",
    "+ torch.optim.LBFGS\n",
    "+ torch.optim.RMSprop\n",
    "+ torch.optim.Rprop\n",
    "\n",
    "而以上这些优化算法均继承于`Optimizer`，下面我们先来看下所有优化器的基类`Optimizer`。定义如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc131945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b62542",
   "metadata": {},
   "source": [
    "\n",
    "**`Optimizer`有三个属性：**\n",
    "\n",
    "+ `defaults`：存储的是优化器的超参数，例子如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d93d3",
   "metadata": {},
   "source": [
    "\n",
    "+ `state`：参数的缓存，例子如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultdict(<class 'dict'>, {tensor([[ 0.3864, -0.0131],\n",
    "        [-0.1911, -0.4511]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01011cac",
   "metadata": {},
   "source": [
    "\n",
    "+ `param_groups`：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，例子如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b722da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'params': [tensor([[-0.1022, -1.6890],[-1.5116, -1.7846]], requires_grad=True)], 'lr': 1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2effb67",
   "metadata": {},
   "source": [
    "\n",
    "**`Optimizer`还有以下的方法：**\n",
    "\n",
    "+ `zero_grad()`：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, set_to_none: bool = False):\n",
    "    for group in self.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:  #梯度不为空\n",
    "                if set_to_none: \n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad.detach_()\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()# 梯度设置为0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727108f8",
   "metadata": {},
   "source": [
    "\n",
    "+ `step()`：执行一步梯度更新，参数更新\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, closure): \n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1af7c",
   "metadata": {},
   "source": [
    "\n",
    "+ `add_param_group()`：添加参数组\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a322014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_param_group(self, param_group):\n",
    "    assert isinstance(param_group, dict), \"param group must be a dict\"\n",
    "# 检查类型是否为tensor\n",
    "    params = param_group['params']\n",
    "    if isinstance(params, torch.Tensor):\n",
    "        param_group['params'] = [params]\n",
    "    elif isinstance(params, set):\n",
    "        raise TypeError('optimizer parameters need to be organized in ordered collections, but '\n",
    "                        'the ordering of tensors in sets will change between runs. Please use a list instead.')\n",
    "    else:\n",
    "        param_group['params'] = list(params)\n",
    "    for param in param_group['params']:\n",
    "        if not isinstance(param, torch.Tensor):\n",
    "            raise TypeError(\"optimizer can only optimize Tensors, \"\n",
    "                            \"but one of the params is \" + torch.typename(param))\n",
    "        if not param.is_leaf:\n",
    "            raise ValueError(\"can't optimize a non-leaf Tensor\")\n",
    "\n",
    "    for name, default in self.defaults.items():\n",
    "        if default is required and name not in param_group:\n",
    "            raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n",
    "                             name)\n",
    "        else:\n",
    "            param_group.setdefault(name, default)\n",
    "\n",
    "    params = param_group['params']\n",
    "    if len(params) != len(set(params)):\n",
    "        warnings.warn(\"optimizer contains a parameter group with duplicate parameters; \"\n",
    "                      \"in future, this will cause an error; \"\n",
    "                      \"see github.com/PyTorch/PyTorch/issues/40967 for more information\", stacklevel=3)\n",
    "# 上面好像都在进行一些类的检测，报Warning和Error\n",
    "    param_set = set()\n",
    "    for group in self.param_groups:\n",
    "        param_set.update(set(group['params']))\n",
    "\n",
    "    if not param_set.isdisjoint(set(param_group['params'])):\n",
    "        raise ValueError(\"some parameters appear in more than one parameter group\")\n",
    "# 添加参数\n",
    "    self.param_groups.append(param_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf5cf5",
   "metadata": {},
   "source": [
    "\n",
    "+ `load_state_dict()` ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71caca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(self, state_dict):\n",
    "    r\"\"\"Loads the optimizer state.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): optimizer state. Should be an object returned\n",
    "            from a call to :meth:`state_dict`.\n",
    "    \"\"\"\n",
    "    # deepcopy, to be consistent with module API\n",
    "    state_dict = deepcopy(state_dict)\n",
    "    # Validate the state_dict\n",
    "    groups = self.param_groups\n",
    "    saved_groups = state_dict['param_groups']\n",
    "\n",
    "    if len(groups) != len(saved_groups):\n",
    "        raise ValueError(\"loaded state dict has a different number of \"\n",
    "                         \"parameter groups\")\n",
    "    param_lens = (len(g['params']) for g in groups)\n",
    "    saved_lens = (len(g['params']) for g in saved_groups)\n",
    "    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n",
    "        raise ValueError(\"loaded state dict contains a parameter group \"\n",
    "                         \"that doesn't match the size of optimizer's group\")\n",
    "\n",
    "    # Update the state\n",
    "    id_map = {old_id: p for old_id, p in\n",
    "              zip(chain.from_iterable((g['params'] for g in saved_groups)),\n",
    "                  chain.from_iterable((g['params'] for g in groups)))}\n",
    "\n",
    "    def cast(param, value):\n",
    "        r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\n",
    "   \t\t.....\n",
    "\n",
    "    # Copy state assigned to params (and cast tensors to appropriate types).\n",
    "    # State that is not assigned to params is copied as is (needed for\n",
    "    # backward compatibility).\n",
    "    state = defaultdict(dict)\n",
    "    for k, v in state_dict['state'].items():\n",
    "        if k in id_map:\n",
    "            param = id_map[k]\n",
    "            state[param] = cast(param, v)\n",
    "        else:\n",
    "            state[k] = v\n",
    "\n",
    "    # Update parameter groups, setting their 'params' value\n",
    "    def update_group(group, new_group):\n",
    "       ...\n",
    "    param_groups = [\n",
    "        update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n",
    "    self.__setstate__({'state': state, 'param_groups': param_groups})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251e9da",
   "metadata": {},
   "source": [
    "\n",
    "+ `state_dict()`：获取优化器当前状态信息字典\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68120be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict(self):\n",
    "    r\"\"\"Returns the state of the optimizer as a :class:`dict`.\n",
    "\n",
    "    It contains two entries:\n",
    "\n",
    "    * state - a dict holding current optimization state. Its content\n",
    "        differs between optimizer classes.\n",
    "    * param_groups - a dict containing all parameter groups\n",
    "    \"\"\"\n",
    "    # Save order indices instead of Tensors\n",
    "    param_mappings = {}\n",
    "    start_index = 0\n",
    "\n",
    "    def pack_group(group):\n",
    "\t\t......\n",
    "    param_groups = [pack_group(g) for g in self.param_groups]\n",
    "    # Remap state to use order indices as keys\n",
    "    packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v\n",
    "                    for k, v in self.state.items()}\n",
    "    return {\n",
    "        'state': packed_state,\n",
    "        'param_groups': param_groups,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b710a3",
   "metadata": {},
   "source": [
    "\n",
    "## 3.9.2 实际操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n",
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)\n",
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# 保存参数信息\n",
    "torch.save(optimizer.state_dict(),os.path.join(r\"D:\\pythonProject\\Attention_Unet\", \"optimizer_state_dict.pkl\"))\n",
    "print(\"----------done-----------\")\n",
    "# 加载参数信息\n",
    "state_dict = torch.load(r\"D:\\pythonProject\\Attention_Unet\\optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "print(\"\\n{}\".format(optimizer.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558c17e",
   "metadata": {},
   "source": [
    "\n",
    "## 3.9.3 输出结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行更新前的数据，梯度\n",
    "The data of weight before step:\n",
    "tensor([[-0.3077, -0.1808],\n",
    "        [-0.7462, -1.5556]])\n",
    "The grad of weight before step:\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "# 进行更新后的数据，梯度\n",
    "The data of weight after step:\n",
    "tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]])\n",
    "The grad of weight after step:\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "# 进行梯度清零的梯度\n",
    "The grad of weight after optimizer.zero_grad():\n",
    "tensor([[0., 0.],\n",
    "        [0., 0.]])\n",
    "# 输出信息\n",
    "optimizer.params_group is \n",
    "[{'params': [tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n",
    "\n",
    "# 证明了优化器的和weight的储存是在一个地方，Python基于值管理\n",
    "weight in optimizer:1841923407424\n",
    "weight in weight:1841923407424\n",
    "    \n",
    "# 输出参数\n",
    "optimizer.param_groups is\n",
    "[{'params': [tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n",
    "        [ 0.6630, -1.5178, -0.8708],\n",
    "        [-2.0222,  1.4573,  0.8657]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n",
    "\n",
    "# 进行更新前的参数查看，用state_dict\n",
    "state_dict before step:\n",
    " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
    "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "# 进行更新后的参数查看，用state_dict\n",
    "state_dict after step:\n",
    " {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "\n",
    "# 存储信息完毕\n",
    "----------done-----------\n",
    "# 加载参数信息成功\n",
    "load state_dict successfully\n",
    "# 加载参数信息\n",
    "{'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "\n",
    "# defaults的属性输出\n",
    "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n",
    "\n",
    "# state属性输出\n",
    "defaultdict(<class 'dict'>, {tensor([[-1.3031, -1.1761],\n",
    "        [-1.7415, -2.5510]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}})\n",
    "\n",
    "# param_groups属性输出\n",
    "[{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [tensor([[-1.3031, -1.1761],\n",
    "        [-1.7415, -2.5510]], requires_grad=True)]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n",
    "        [ 0.6630, -1.5178, -0.8708],\n",
    "        [-2.0222,  1.4573,  0.8657]], requires_grad=True)]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbf6e9",
   "metadata": {},
   "source": [
    "\n",
    "**注意：**\n",
    "\n",
    "1. 每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9986878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Moddule):\n",
    "    ···\n",
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(),lr=lr)\n",
    "optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a549ea",
   "metadata": {},
   "source": [
    "\n",
    "2. optimizer在一个神经网络的epoch中需要实现下面两个步骤：\n",
    "   1. 梯度置零\n",
    "   2. 梯度更新\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcade383",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "for epoch in range(EPOCH):\n",
    "\t...\n",
    "\toptimizer.zero_grad()  #梯度置零\n",
    "\tloss = ...             #计算loss\n",
    "\tloss.backward()        #BP反向传播\n",
    "\toptimizer.step()       #梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c959543",
   "metadata": {},
   "source": [
    "3. 给网络不同的层赋予不同的优化器参数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "net = resnet18()\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params':net.fc.parameters()},#fc的lr使用默认的1e-5\n",
    "    {'params':net.layer4[0].conv1.parameters(),'lr':1e-2}],lr=1e-5)\n",
    "\n",
    "# 可以使用param_groups查看属性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f303e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.9.4 实验\n",
    "\n",
    "为了更好的帮大家了解优化器，我们对PyTorch中的优化器进行了一个小测试\n",
    "\n",
    "**数据生成**：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dcf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(-1, 1, 1000)\n",
    "# 升维操作\n",
    "x = torch.unsqueeze(a, dim=1)\n",
    "y = x.pow(2) + 0.1 * torch.normal(torch.zeros(x.size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5be71",
   "metadata": {},
   "source": [
    "\n",
    "**数据分布曲线**：\n",
    "\n",
    "![](./figures/3.6.1.png)\n",
    "\n",
    "**网络结构**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 20)\n",
    "        self.predict = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86234276",
   "metadata": {},
   "source": [
    "# 第三章：PyTorch的主要组成模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "3.1 思考：完成深度学习的必要部分\n",
    "3.2 基本配置\n",
    "3.3 数据读入\n",
    "3.4 模型构建\n",
    "3.5 模型初始化\n",
    "3.6 损失函数\n",
    "3.7 训练与评估\n",
    "3.8 可视化\n",
    "3.9 优化器\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
